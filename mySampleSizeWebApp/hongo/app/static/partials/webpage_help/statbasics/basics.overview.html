<h2 class="text-grey text-lowercase">Overview</h2>
<hr>
<div class="row margin-bottom-2em">
    <div class="col-sm-2"><span class="fa-4x"><i class="fa fa-newspaper-o text-green"></i></span></div>
    <div class="col-sm-10">
        <p class="text-left">
            Once you’ve decided on treatments and settings, factors, and outcome measures to explore your scientific conjecture, the appropriate number of subjects is the crucial remaining decision. One way to select and support a choice on the number of subjects involves prospective statistical consideration of power and significance.  There are four key quantities you need for this process: significance, power, outcome standard deviation, and effect size.
        </p>
    </div>
</div>

<h3 class="text-grey text-lowercase padding-left-2em">Significance and Power</h3>
<div class="row margin-bottom-2em">
    <div class="col-sm-2"><span class="padding-left-1em fa-4x"><i class="fa fa-line-chart text-green"></i></span></div>
    <div class="col-sm-10">
        <p>
            Significance is the probability of finding a false positive in an experiment, while power is the probability of finding a true positive.  By false positive, we mean that the experiment’s outcome indicates the experimental conditions generate an effect when the scientific truth is that of no effect. True positive means that the experiment’s outcome indicates the experimental conditions generate an effect when the scientific truth is that there is a real effect.
        </p>
        <p>
            Probabilities and uncertainty enter due to subject-to-subject variation inherent in life and social science research. Since we must make a decision about our conjecture based on uncertain evidence, the possibility of erroneous interpretation exists. The two errors we can make are
            <ol>
                <li> Declaring we have found an effect when there is none, and </li>
                <li> Declaring we have failed to find an effect where an effect is actually present.</li>
            </ol>
        </p>
        <p>
            These errors are called Type I and Type II errors, respectively. Translation of these into statistical language is
            <ol>
                <li> Rejecting the null hypothesis when the null hypothesis is true, and </li>
                <li> Failing to reject the null hypothesis when the null hypothesis is false. </li>
            </ol>
        </p>

        <p>
            In a sense, this language mirrors the legal language of “guilty” vs “not guilty.”  When we say that we fail to reject the null hypothesis, we are saying “not guilty,” as opposed to saying “innocent,” which would correspond to accepting the null hypothesis. To wander into the weeds a bit, we might invoke the philosopher of science Karl Popper, who distinguished falsifiability and verifiability: we may provide evidence that the null is false, but verifying the null hypothesis is a different matter. 
        </p>
        <p>
            Quantifying the likelihood of errors of interpretation uses the language of probability. The Type I error probability, sometimes called significance or α, is the likelihood of declaring an effect when none exists, and the Type II error probability, called β, is the chance of failing to find evidence of an effect that actually exists.  Another quantity we will refer to quite a bit in the sequel is the power of a hypothesis test, 1 - β, which is the likelihood of detecting an effect that actually is present. The table below illustrates these concepts. 
            
            <table></table>
        </p>
        
        <p>
            Designing an experiment requires making choices that keep both of these error rates low. The most common approach to design begins with the specification of an acceptable Type I error rate or significance level α.  It is very common in the scientific literature to see <i>α</i> = 0.05 (or 5% significance) used; however, this choice is a somewhat arbitrary selection based largely on a comment of the famous statistician Sir Ronald Fisher in his 1925 text <i>Statistical Methods for Research Workers</i>. 
        </p>
        <p>
            Some researchers (reference) are recommending decreasing the significance to <i>α</i> = 0.005 (or 0.5% significance) to improve reproducibility.  This condition is a more stringent one for declaring a positive experimental result, but it too remains a somewhat arbitrary cut-off.  Please see ASA statement (reference) for more discussion on this challenging matter. 
        </p>
        <p>
            The Type II error rate <i>β</i> or the power 1 – <i>β</i> must also be specified for statistical considerations to aid with experimental design. A common choice that we see in NIH proposals is <i>β</i> = 0.20 or equivalently 1 – <i>β</i> = 0.80.  Again, these are arbitrary but widely accepted choices. 
            <br>
            Good practice is to explore the dependence of the experimental design on these specifications.
        </p>
        <p class="black-box-emphasis"> 
            The significance and power, as probabilities, are unitless quantities with values between 0 and 1.
        </p>
    </div>
</div>

<h3 class="text-grey text-lowercase padding-left-2em">Mean, Standard Deviation, and Effect Size</h3>
<div class="row margin-bottom-2em">
    <div class="col-sm-2"><span class="padding-left-1em fa-4x"><i class="fa fa-line-chart text-green"></i></span></div>
    <div class="col-sm-10">
        <p>
            Consider an experiment to study the effects of metabotropic glutamate receptor agonist, DCG-IV, on acute neuronal degeneration after traumatic brain injury in rats. Doses of 0 and 100 fmol of the treatment were administered to two groups of subjects. The 0 dose group had 7 subjects, while the 100 fmol dose group had 6. The number of degenerated brain cells in a tissue sample were counted. In the figure, we see that the individual subjects in each group showed quite a lot of variation in outcome measures, and we also see what appears to be an "overall" decrease in the number of degenerated brain cells from the 0 dose to the 100 fmol dose. 
            <br>
            Is the difference “real”? Or is the variation between the subjects so great we’re seeing a decrease when it’s not there?  Statistical methods can help us separate the “signal” of the difference from the “noise” of variation among the subjects.
        </p>
        
        <div class="row padding-bottom-1em">
            <div class="col-sm-8 col-sm-offset-2">
                <img ng-src="{{imgPath}}/doseResponseVert0_100.png" class="width-100" src="{{imgPath}}/doseResponseVert0_100.png" spinner-img-load>
            </div>
        </div>
        
        <p>
            The dose-response figure illustrates some important principles for applying statistical techniques in the scientific process. The statistical side of the scientific theory posits that, at each dose level, the population of all possible subject outcome measures has a mean or central value, usually called µ. Around that central value, individual outcomes disperse, some close and some far from the center. The standard deviation, σ, is a measure of how much dispersion or spread the population has. 
            
            Roughly speaking, the standard deviation is the average distance from the individual outcome measures to the population mean. It is important to note that the mean and standard deviation have the same units as the outcome measure they are used to model.
        </p>
        <p>
            When we consider the figure above in terms of the statistical components of the scientific project, the standard deviation is responsible for the vertical data spread that arises from between-subjects variation. Differences in population means from the differences in drug dosage applied to the subject groups are responsible for the horizontal variation. The statistical question translated from the scientific question is this: is the horizontal variation big enough that it is inconsistent with the null hypothesis of no treatment effect? 
        </p>
        <p>
            The term “effect size” relates to the differences among population means. Its specific definition, in terms of a formula, depends on the type of experiment (more on this later). In the case of an experiment that compares two separate groups of subjects, the effect size is exactly the difference in population means. 
            
            <b>The heart of a statistical hypothesis test is evaluating evidence that the effect size is not zero.</b>
        </p>
        <p class="black-box-emphasis">
            The standard deviation and effect size have the same units as the outcome measure of the experiment.
        </p>
        <p>
            The “magic number” in experimental design is the signal to noise ratio:
            <div katex>
                \frac{ \sqrt{\text{number of subjects}} \cdot \text{effect size}}{ \text{ standard deviation } }
            </div>
        </p>
        <p>
            When you select a significance level, power increases with this signal to noise ratio, or, more explicitly, power increases when
            <ul>
                <li> The number of subjects increases, </li>
                <li> The effect size increases, or </li>
                <li> The standard deviation decrease. </li>
            </ul>
            It is also important to note that power increases when significance increases: if you are more tolerant of false positives, you will also be more likely to detect true positives.
        </p>
    </div>
</div>
